{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83d\uddfa\ufe0f SMapper","text":"<p>A Portable Multimodal Sensing Platform Desiged for Robotics Research</p> <p></p> <p>SMapper is a portable, multimodal sensor platform designed from the ground up to accelerate robotics research, particularly for Simultaneous Localization and Mapping (SLAM) applications. It was created to bridge the gap between expensive, inflexible commercial systems and unreliable do-it-yourself solutions, offering a versatile, high-performance, and user-friendly tool for collecting high-quality, time-synchronized sensor data.</p> <p>This documentation serves as the central resource for understanding, operating, and developing with the SMapper platform.</p>"},{"location":"#the-platform-at-a-glance","title":"The Platform at a Glance","text":"<p>SMapper is an integrated system of carefully selected hardware and custom-built software designed for reliability and ease of use in research environments.</p> <ul> <li> <p>Hardware: The core of the device features a powerful NVIDIA Jetson Orin AGX processing unit.   For sensing, it is equipped with a 3D LiDAR with a wide vertical field of view,   a panoramic array of four high-resolution 20MP cameras, and an Intel RealSense depth/inertial camera.   The entire system is housed in a robust, 3D-printed enclosure and powered by a field-swappable battery,   enabling approximately one hour of continuous operation.</p> </li> <li> <p>Software: A complete, full-stack application running on ROS 2 Humble provides   total control over the platform. This includes a custom, high-performance C++ camera driver (<code>argus_camera_ros</code>)   for precise data acquisition, a backend RESTful API (<code>smapper_api</code>) for system control,   and an intuitive React web interface (<code>smapper_app</code>) for monitoring, data management, and visualization.</p> </li> <li> <p>Calibration: A key feature of the project is a rigorous, automated calibration pipeline.   Using the custom <code>smapper_toolbox</code>, the system can accurately determine the intrinsic   and extrinsic parameters of all sensors, generating a ready-to-use configuration for high-precision data fusion.</p> </li> </ul>"},{"location":"#purpose-of-this-documentation","title":"Purpose of This Documentation","text":"<p>This site is intended to provide all the necessary information for users and developers. Here you will find:</p> <ul> <li> <p>Getting Started Guides: Step-by-step instructions for setting up, operating, and collecting data with SMapper.</p> </li> <li> <p>System Architecture: Detailed descriptions of the hardware components, mechanical design, and software stack.</p> </li> <li> <p>Software Documentation: In-depth information on the custom software components, including the API endpoints and driver configurations.</p> </li> <li> <p>Calibration Pipeline: A complete guide to using the automated <code>smapper_toolbox</code> to calibrate the device.</p> </li> </ul> <p>Whether you are a new user looking to collect your first dataset or a developer aiming to extend the platform's capabilities, this is the place to begin.</p>"},{"location":"publications/","title":"\ud83d\udcd6 Publications","text":"<p>Each publication is a milestone on the journey of turning ideas into shared knowledge.</p> <p>Citing Our Work</p> <p>We kindly ask that you cite the relevant paper(s) below if you have used any of our datasets in your research.</p>"},{"location":"publications/#papers","title":"Papers","text":"Preprint | SMapper: A Multi-Modal Data Acquisition Platform for SLAM Benchmarking Abstract <p>Advancing research in fields like Simultaneous Localization and Mapping (SLAM) and autonomous navigation critically depends on reliable and reproducible multimodal datasets. While several influential datasets have driven progress in these domains, they often suffer from limitations in sensing modalities, environmental diversity, and the reproducibility of the underlying hardware setups. To address these challenges, this paper introduces SMapper, a novel open-hardware, multi-sensor platform designed explicitly for, though not limited to, SLAM research. The device integrates synchronized LiDAR, multi-camera, and inertial sensing, supported by a robust calibration and synchronization pipeline that ensures precise spatio-temporal alignment across modalities. Its open and replicable design allows researchers to extend its capabilities and reproduce experiments across both handheld and robot-mounted scenarios. To demonstrate its practicality, we additionally release SMapper-light, a publicly available SLAM dataset containing representative indoor and outdoor sequences. The dataset includes tightly synchronized multimodal data and ground-truth trajectories derived from offline LiDAR-based SLAM with sub-centimeter accuracy, alongside dense 3D reconstructions. Furthermore, the paper contains benchmarking results on state-of-the-art LiDAR and visual SLAM frameworks using the SMapper-light dataset. By combining open-hardware design, reproducible data collection, and comprehensive benchmarking, SMapper establishes a robust foundation for advancing SLAM algorithm development, evaluation, and reproducibility.</p> citation<pre><code>@misc{soares2025smappermultimodaldataacquisition,\n      title={SMapper: A Multi-Modal Data Acquisition Platform for SLAM Benchmarking},\n      author={Pedro Miguel Bastos Soares and Ali Tourani and Miguel Fernandez-Cortizas and Asier Bikandi Noya and Jose Luis Sanchez-Lopez and Holger Voos},\n      year={2025},\n      eprint={2509.09509},\n      archivePrefix={arXiv},\n      primaryClass={cs.RO},\n      url={https://arxiv.org/abs/2509.09509},\n}\n</code></pre>"},{"location":"components/","title":"\ud83e\udde9 Components","text":"<p>\ud83d\udd0e What's Inside? \ud83d\udc40</p> <p></p> <p>To support its high-performance, real-time capabilities, SMapper is built with a carefully selected set of components, each playing a crucial role in data acquisition and processing. The exploded view below offers a peek inside the device:</p> <p></p>"},{"location":"components/#core-components","title":"\ud83d\udd27 Core Components:","text":"<ul> <li> <p>Jetson AGX Orin Developer Kit<sup>1</sup></p> <p>The brain of the system, handling real-time sensor synchronization, data capture, and onboard processing.</p> </li> <li> <p>4x 20MP e-CAM200_CUOAGX by e-Con Systems<sup>2</sup></p> <p>Featuring 118.81\u00b0 FoV and rolling shutter sensors, these high-resolution cameras are synchronized and arranged for overlapping coverage.</p> </li> <li> <p>Ouster OS0-64 LiDAR<sup>3</sup></p> <p>A compact yet powerful 3D LiDAR unit providing dense point clouds, ideal for close-range mapping and SLAM integration.</p> </li> <li> <p>Intel RealSense D435i<sup>4</sup></p> <p>Adds RGB-D data and IMU readings, expanding the sensory depth for cases only relying on RGB-D datasets.</p> </li> </ul>"},{"location":"components/#power-connectivity","title":"\ud83d\udd0b Power &amp; Connectivity","text":"<ul> <li> <p>20V 6.0Ah Drill Battery</p> <p>Easily swappable and rechargeable\u2014ideal for long field sessions.</p> </li> <li> <p>Buck Converter</p> <p>Steps down battery voltage to power the Jetson and peripherals efficiently.</p> </li> <li> <p>DC-Jack to USB-C Male Adapter</p> <p>Delivers power to the Jetson using standard interfaces.</p> </li> <li> <p>USB 3.0 Right-Angle Adapter</p> <p>For compact, secure, and clean cable management inside the enclosure.</p> </li> </ul>"},{"location":"components/#design-mounting-system","title":"\ud83d\udee0\ufe0f Design &amp; Mounting System","text":"<p>SMapper is designed not only for performance, but for usability in the field. Its custom-built physical design brings together modularity, thermal efficiency, and versatility in deployment, whether handheld or robot-mounted.</p>"},{"location":"components/#3d-printed-enclosure","title":"3D-Printed Enclosure","text":"<p>The body of SMapper is split into three parts which are bolted together to form a robust enclosure:</p> <ul> <li> <p>Bottom shell \u2013 Houses the power module and the Jetson AGX Orin.</p> </li> <li> <p>Middle shell \u2013 The main body of the enclosure, where the cameras, including the realsense are mounted to.</p> </li> <li> <p>Top shell \u2013 Optimized for SLM metal 3D printing, this part helps dissipate heat from the LiDAR during continuous operation.</p> </li> </ul> <p>\ud83d\udcf7 (Insert close-up of the enclosure with shell labels here)</p>"},{"location":"components/#coupling-attachment-mechanism","title":"Coupling &amp; Attachment Mechanism","text":"<p>The base of the system features a custom sliding coupling that enables quick attachment/detachment to different platforms:</p> <ul> <li>Handheld configuration with the ergonomic grip and battery mount.</li> <li>Robot-mount configuration, adaptable to a variety of mobile bases, including legged robots.</li> </ul> <p>\ud83d\udcf7 (Insert photos of the coupling system sliding into both grip and robot mount here)</p>"},{"location":"components/#ergonomic-handle","title":"Ergonomic Handle","text":"<p>Designed with field use in mind, the handle balances the weight of the ~2kg device evenly thanks to its integration with a standard 20V drill battery, keeping the center of mass low and making the device easy to carry and operate during long sessions.</p> <p>\ud83d\udcf7 (Insert photo of the handheld setup in action here)</p> <ol> <li> <p>https://developer.nvidia.com/embedded/learn/jetson-agx-orin-devkit-user-guide/index.html\u00a0\u21a9</p> </li> <li> <p>https://www.e-consystems.com/nvidia-cameras/jetson-agx-orin-cameras/20mp-ar2020-high-resolution-mipi-camera.asp\u00a0\u21a9</p> </li> <li> <p>https://ouster.com/products/hardware/os0-lidar-sensor\u00a0\u21a9</p> </li> <li> <p>https://www.intelrealsense.com/depth-camera-d435i/\u00a0\u21a9</p> </li> </ol>"},{"location":"components/cad/","title":"CAD Files","text":"<p>All the CAD files are available at snt-arg/smapper/hardware.</p> <p>In this folder you\u2019ll find both STEP and STL files:</p> <ul> <li>STEP \u2192 editable CAD models (open in Fusion 360, SolidWorks, FreeCAD, etc.)</li> <li>STL \u2192 ready-to-slice models for 3D printing</li> </ul>"},{"location":"components/cad/#how-to-use","title":"How to Use","text":"<ul> <li>If you want to modify the design, open the <code>.step</code> files in your preferred CAD software.</li> <li>If you want to print parts directly, load the <code>.stl</code> files into your slicer of choice (e.g., PrusaSlicer, Cura).</li> </ul>"},{"location":"components/cad/#preview","title":"Preview","text":""},{"location":"components/sensors/","title":"Sensors Placement","text":""},{"location":"datasets/","title":"About","text":"<p>A collection of datasets recorded with SMapper</p> <p>Datasets recorded with SMapper are stored as ROS 2 bags. This means they follow a standard format, are easy to play back, and can be used with many existing tools that already support rosbags or can convert them into other formats.</p> <p>All datasets here are stored using the MCAP format. This is now the default in ROS 2 Jazzy and brings some advantages compared to the traditional SQLite storage backend.</p> <p>If you\u2019re on an older ROS 2 distribution, you might need to install MCAP support manually. You can install the package like this:</p> <pre><code>sudo apt install ros-$ROS_DISTRO-rosbag2-storage-mcap\n</code></pre>"},{"location":"datasets/#topics-in-each-dataset","title":"Topics in Each Dataset","text":"<p>Every dataset includes the following topics. These cover 3D point clouds, IMU data from two different sources, multiple camera streams, and the required transforms.</p> Topic Name Message Type Description Frequency [Hz] <code>/ouster/points</code> <code>sensor_msgs/PointCloud2</code> 3D point cloud data captured by the Ouster OS0-64 LiDAR. 10 <code>/ouster/imu</code> <code>sensor_msgs/Imu</code> Accelerometer and gyroscope data from the Ouster IMU. 100 <code>/camera/realsense/imu</code> <code>sensor_msgs/Imu</code> Accelerometer and gyroscope data from the Realsense D435i IMU. 400 <code>/camera/realsense/depth/image_raw</code> <code>sensor_msgs/Image</code> Raw depth images from the Realsense D435i. 30 <code>/camera/realsense/depth/camera_info</code> <code>sensor_msgs/CameraInfo</code> Camera info for the depth stream of the Realsense D435i. 30 <code>/camera/realsense/color/image_raw</code> <code>sensor_msgs/Image</code> Raw color images from the Realsense D435i. 30 <code>/camera/realsense/color/camera_info</code> <code>sensor_msgs/CameraInfo</code> Camera info for the color stream of the Realsense D435i. 30 <code>/camera/realsense/aligned_depth_to_color/image_raw</code> <code>sensor_msgs/Image</code> Depth images aligned to the color stream of the Realsense D435i. 30 <code>/camera/front_right/image_raw</code> <code>sensor_msgs/Image</code> Raw images from the front-right high-resolution Argus camera. 30 <code>/camera/front_left/image_raw</code> <code>sensor_msgs/Image</code> Raw images from the front-left high-resolution Argus camera. 30 <code>/camera/side_right/image_raw</code> <code>sensor_msgs/Image</code> Raw images from the side-right high-resolution Argus camera. 30 <code>/camera/side_left/image_raw</code> <code>sensor_msgs/Image</code> Raw images from the side-left high-resolution Argus camera. 30 <code>/camera/front_right/camera_info</code> <code>sensor_msgs/CameraInfo</code> Camera info for the front-right high-resolution Argus camera. 30 <code>/camera/front_left/camera_info</code> <code>sensor_msgs/CameraInfo</code> Camera info for the front-left high-resolution Argus camera. 30 <code>/camera/side_right/camera_info</code> <code>sensor_msgs/CameraInfo</code> Camera info for the side-right high-resolution Argus camera. 30 <code>/camera/side_left/camera_info</code> <code>sensor_msgs/CameraInfo</code> Camera info for the side-left high-resolution Argus camera. 30 <code>/tf</code> <code>tf2_msgs/tfMessage</code> Transformation frames (dynamic). \u2013 <code>/tf_static</code> <code>tf2_msgs/tfMessage</code> Transformation frames (static). \u2013"},{"location":"datasets/smapper-light/","title":"SMapper Light Dataset","text":"<p>SMapper light is a publicly available multimodal dataset collected using the SMapper platform, an open-hardware, multi-sensor device designed for SLAM (Simultaneous Localization and Mapping) research. The dataset provides 3D LiDAR, multi-camera, and IMU measurements, enabling benchmarking of visual, LiDAR, and visual\u2013inertial SLAM methods.</p>"},{"location":"datasets/smapper-light/#applications","title":"\ud83d\ude80 Applications","text":"<p>SMapper light dataset can be used for:</p> <ul> <li>Benchmarking LiDAR SLAM frameworks (e.g., Fast-LIO, S-Graphs)</li> <li>Benchmarking Visual SLAM frameworks (e.g., ORB-SLAM3, vS-Graphs)</li> <li>Benchmarking Multimodal SLAM frameworks</li> </ul>"},{"location":"datasets/smapper-light/#dataset-overview","title":"\ud83d\udcca Dataset Overview","text":"Scenario Instance Duration Size on Disk Description Indoor <code>IN_SMALL_01</code> 01 m 29 s 5.7 GB Confined single-room environment <code>IN_MULTI_01</code> 06 m 46 s 13.5 GB Multi-room linear trajectory <code>IN_MULTI_02</code> 07 m 07 s 13.0 GB Multi-room with loop closure <code>IN_LARGE_01</code> 09 m 30 s 57.9 GB Large-scale indoor with loop closure Outdoor <code>OUT_CAMPUS_01</code> 04 m 57 s 36.4 GB Urban campus linear path <code>OUT_CAMPUS_02</code> 05 m 15 s 37.8 GB Urban campus circular path Total 35 m 04 s 164.3 GB <p>Info</p> <p>The available topics can be found in here</p>"},{"location":"guides/calibration/","title":"\ud83d\udcd0 Device Calibration","text":"<p>Device calibration is a critical step for accurate sensor fusion and robust performance. This project uses the Kalibr toolbox to calibrate the intrinsic and extrinsic parameters of each camera. We also estimate the IMU noise model for the Ouster OS0 using Allan variance analysis and compute the spatial transformation between each camera and the OS0's IMU frame.</p> <p>To streamline this process, we have developed the smapper_toolbox, an automated calibration toolbox that significantly reduces setup time and ensures reproducibility.</p>"},{"location":"guides/calibration/#pre-requisites","title":"Pre-requisites","text":"<p>A calibration target is required to perform the calibration. The recommended target is the <code>Aprilgrid 6x6 0.8x0.8 m (A0 page)</code>, which can be downloaded here. For best results, mount the target on a rigid panel with a matte finish to prevent reflections.</p>"},{"location":"guides/calibration/#calibration-process","title":"Calibration Process","text":""},{"location":"guides/calibration/#data-collection","title":"Data Collection","text":"<ol> <li>Position the calibration target. Place it vertically, for instance, against a wall, at a height that is comfortable for data collection.</li> <li>Collect IMU data. Record a rosbag containing only IMU data from both the Ouster and Realsense IMUs. It is recommended to collect at least 3 hours of data, but more is better (e.g., 20 hours is more than sufficient). Place the device in a static position and record the IMU data for each sensor independently.</li> <li>Collect dynamic data for each camera. For each camera, including the Realsense RGB sensor, record a dynamic rosbag. Each rosbag should contain one camera stream and the Ouster and/or Realsense IMU data.<ul> <li>Procedure: Start recording with the camera facing the target. Exert all axes of the IMU by rotating the device on its pitch, roll, and yaw axes. Then, perform translational movements: up and down, left to right, and forward and backward. Try to keep the target in view at all times. This video tutorial provides an excellent demonstration of the process.</li> </ul> </li> <li>(Optional) Collect stereo camera data. You can collect another rosbag with the two front cameras and one or more IMU sources. This is useful for stereo vision applications. Repeat the procedure from the previous step, ensuring that both cameras capture some of the same features.</li> </ol>"},{"location":"guides/calibration/#folder-structure","title":"Folder Structure","text":"<p>After collecting all the bags, organize them into the following folder structure:</p> <pre><code>.\n\u2514\u2500\u2500 ros2\n    \u251c\u2500\u2500 calib02_front_left\n    \u251c\u2500\u2500 calib02_front_right\n    \u251c\u2500\u2500 calib02_side_left\n    \u251c\u2500\u2500 calib02_side_right\n    \u2514\u2500\u2500 imu16h_imu\n</code></pre> <p>Warning</p> <p>This folder structure is crucial for the automation toolbox to function correctly. You must have a folder named <code>ros2</code> containing the ros2 bags. The toolbox will then create a <code>ros1</code> folder with the converted bags for Kalibr.</p>"},{"location":"guides/calibration/#using-the-smapper_toolbox","title":"Using the <code>smapper_toolbox</code>","text":"<p>Important</p> <p>You must have the <code>smapper</code> repository on your system, as it contains required files. Cloning it also clones the toolbox. Additionally, <code>smapper_toolbox</code> uses the UV Python package manager. Please refer to the smapper_toolbox documentation for more information.</p> <p>\u26a0\ufe0f These following steps must be performed on an X86 CPU due to docker image constraints!</p> <ol> <li>Navigate to the <code>smapper_toolbox</code> directory, which should be located at <code>smapper/software/smapper_toolbox</code>.</li> <li>View the help menu by running <code>uv run ./toolbox --help</code>.</li> <li>Modify configuration accordingly to your needs. Configuration lives in <code>smapper_toolbox/config/config.yaml</code></li> <li>Run the calibration pipeline with the following command:     <pre><code># The path to the rosbags is the root directory where the ros2 folder is located.\n# You can also run --help to see all available options.\nuv run ./toolbox kalibr all --rosbags-dir path/to/where/rosbags/are/\n</code></pre>     This command executes a two-stage process. First, it converts all ROS2 bags to ROS1 bags, which can take some time depending on their size. This creates a <code>ros1</code> folder next to the <code>ros2</code> folder. Then, it proceeds to the Kalibr stage, where it extracts basic metadata from the available bags, runs the intrinsics calibration for each bag containing one or more image topics, computes the noise model of the IMUs (if there are bags with sufficient IMU data), and finally performs the camera-IMU calibration. All results are saved in the calibration directory specified in the configuration file.</li> <li>Generate the transformation tree. This step requires manual intervention. You will need to go through each camera's result from the camera-IMU step and fill in the following file.</li> <li>Generate the transformations. Once you have filled in the file, run the following command:     <pre><code>uv run ./toolbox transforms generate --input the_file.yaml --output output_tfs.yaml\n</code></pre>     This command reads the file and generates the transformations so that all sensors are referenced to a common <code>base_link</code>. It also generates a ROS launch file that can be used directly to publish all the transformations.</li> <li>Final steps. Copy the launch file to the actual device in <code>smapper/software/ros_ws/src/smapper_bringup/launch</code> and create or update the <code>camera_info</code> files located in <code>smapper/software/ros_ws/src/smapper_bringup/config/calib</code>.</li> </ol>"},{"location":"guides/external_disk/","title":"Transferring Data to an External Disk","text":"<p>This guide details the recommended procedure for copying large ROS bags from the SMapper's internal storage to an external USB disk. This method is significantly faster than downloading files through the web application, especially for large datasets.</p>"},{"location":"guides/external_disk/#prerequisites","title":"Prerequisites","text":"<ul> <li>An external USB disk, preferably formatted with a Linux-native filesystem like ext4 to support file permissions correctly.</li> </ul>"},{"location":"guides/external_disk/#step-1-identify-the-external-disk","title":"Step 1: Identify the External Disk","text":"<p>Before mounting, you must correctly identify the device path for your external disk. Using the wrong path can lead to data loss.</p> <ol> <li> <p>Connect the external disk to one of the device's USB ports.</p> </li> <li> <p>Run the lsblk (list block devices) command to see all connected storage devices.</p> <pre><code>lsblk\n</code></pre> </li> <li> <p>Examine the output to find your disk. You can typically identify it by its SIZE and NAME. New devices usually appear as <code>/dev/sda</code>, <code>/dev/sdb</code>, etc.     The partition you want to mount will be listed underneath, such as <code>/dev/sda1</code>.</p> <pre><code>```\nExample Output:\n\nNAME        MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT\n...\nsda           8:0    0   1.8T  0 disk\n\u2514\u2500sda1        8:1    0   1.8T  0 part\nnvme0n1     259:0    0 465.8G  0 disk\n\u251c\u2500nvme0n1p1 259:1    0   100M  0 part /boot/efi\n...\n```\n</code></pre> <p>In this example, the internal drive is nvme0n1, and the 1.8TB external disk is sda, with the partition to mount being /dev/sda1.</p> <p>Note</p> <p>Throughout this guide, we will use /dev/sda1 as an example. Replace it with the actual device path you identified.</p> </li> </ol>"},{"location":"guides/external_disk/#step-2-create-a-mount-point","title":"Step 2: Create a Mount Point","text":"<p>Note</p> <p>This step only needs to be performed once</p> <p>A mount point is the directory where the files on your external disk will be accessible. We will create a dedicated directory for this purpose.</p> <p>Create the directory:</p> <pre><code>sudo mkdir -p /media/smapper/smapper_ssd\n</code></pre> <p>Set the correct permissions. This command changes the directory's owner to the default user (UID 1000), allowing you to read and write files without using sudo for every operation.</p> <pre><code>sudo chown 1000:1000 -R /media/smapper/smapper_ssd\n</code></pre>"},{"location":"guides/external_disk/#step-3-mount-the-disk","title":"Step 3: Mount the Disk","text":"<p>Now, attach the external disk's filesystem to the mount point you just created.</p> <pre><code>sudo mount /dev/sda1 /media/smapper/smapper_ssd\n</code></pre> <p>You can verify that the disk is mounted correctly by running <code>lsblk</code> again or by using the <code>df -h</code> command.</p>"},{"location":"guides/external_disk/#step-4-copy-your-data","title":"Step 4: Copy Your Data","text":"<p>You can now copy your ROS bags to the external disk. The bags are typically stored in <code>/bags</code>.</p>"},{"location":"guides/external_disk/#step-5-safely-unmount-the-disk","title":"Step 5: Safely Unmount the Disk","text":"<p>Warning</p> <p>This is a critical step. You must unmount the disk before unplugging it. Failing to do so can result in incomplete file transfers and data corruption.</p> <p>Run the umount command to safely detach the disk from the filesystem.</p> <pre><code>sudo umount /media/smapper/smapper_ssd\n</code></pre> <p>After the command completes without errors, you can safely unplug the external disk.</p>"},{"location":"guides/jetpack/","title":"Flashing the Jetson Device","text":"<p>This guide covers the process of flashing the NVIDIA Jetson AGX Orin module with the required Jetpack OS. A specific version and kernel are necessary to ensure compatibility with the e-con Systems camera array.</p> <p>Physical Access Required</p> <p>This entire process must be performed with the Jetson module on a workbench, not mounted inside the SMapper device. Flashing requires access to a USB-C port that is inaccessible once the module is installed.</p>"},{"location":"guides/jetpack/#method-1-flashing-jetpack-60-recommended-stable","title":"Method 1: Flashing Jetpack 6.0 (Recommended &amp; Stable)","text":"<p>This is the official, supported method for ensuring camera compatibility. The kernel patches provided by e-con Systems are built specifically for Jetpack 6.0, making this the most reliable path.</p> <p>The easiest way to install Jetpack 6.0 is by using the NVIDIA SDK Manager on a host computer (Linux).</p>"},{"location":"guides/jetpack/#flashing-steps","title":"Flashing Steps:","text":"<ol> <li>Download &amp; Install: Download and install the NVIDIA SDK Manager on your host machine.</li> <li>Connect the Device: Connect the Jetson AGX Orin to your host machine via the USB-C port. You will need to put the device into Force Recovery Mode.</li> <li>Launch SDK Manager: Start the SDK Manager application. It should automatically detect the connected Jetson device.</li> <li>Select Jetpack Version: In the target hardware configuration step, deselect the latest version and explicitly choose Jetpack 6.0 from the dropdown menu.</li> <li>Start Flashing: Proceed with the on-screen instructions to download the necessary files and flash the operating system to the Jetson module. This process can take a significant amount of time.</li> <li>Post-Installation: Once flashing is complete, the device can be set up Initial Device Setup guide.</li> </ol>"},{"location":"guides/jetpack/#method-2-guide-for-patching-jetpack-62-experimental","title":"Method 2: Guide for Patching Jetpack 6.2 (Experimental)","text":"<p>This is an advanced, experimental guide.</p> <p>While Jetpack 6.2 offers significant camera performance improvements, the official e-con Systems kernel patch is not directly compatible. The following steps are based on a guide provided by e-con Systems to manually patch the 6.2 kernel. Previous attempts to follow this guide were not successful. This section is preserved for documentation purposes and for developers who wish to continue this work.</p>"},{"location":"guides/jetpack/#overview","title":"Overview","text":"<p>This guide details the process of flashing an NVIDIA Jetson AGX Orin development kit with a custom kernel to support the <code>e-CAM200_CUOAGX</code> camera array from e-con Systems.</p> <p>The official e-con Systems kernel patches are built for JetPack 6.0. This guide uses JetPack 6.2, which provides performance boost for cameras. We will download the necessary NVIDIA sources, apply the e-con Systems patch, build the kernel, and flash it to the device's NVMe SSD.</p>"},{"location":"guides/jetpack/#1-setup-host-environment","title":"1. Setup Host Environment \ud83d\udee0\ufe0f","text":"<p>First, we'll set up the development environment on your x86_64 host PC running Ubuntu. All the compilation and flashing operations will happen here.</p>"},{"location":"guides/jetpack/#11-export-environment-variables","title":"1.1 Export Environment Variables","text":"<p>Open a terminal and run the following commands. These variables will simplify the commands in the subsequent steps.</p> <p>Keep This Terminal Open</p> <p>All subsequent commands must be run in this same terminal session. If you close it, you'll need to export these variables again.</p> <pre><code># Top-level directory for all our files\nexport TOP_DIR=$HOME/Downloads/jetpack_orin\n\n# Path to the unpacked e-con Systems driver package\nexport RELEASE_PACK_DIR=$TOP_DIR/e-CAM200_CUOAGX_JETSON_AGX_ORIN_L4T36.3.0_15-May-2024_R02_RC1\n\n# Path to the unpacked Jetson Linux Driver Package\nexport L4T_DIR=$TOP_DIR/Linux_for_Tegra\n\n# Path to the target root filesystem\nexport LDK_ROOTFS_DIR=$TOP_DIR/Linux_for_Tegra/rootfs\n\n# Architecture and cross-compiler toolchain settings\nexport ARCH=arm64\nexport CROSS_COMPILE=$TOP_DIR/tool_chain/aarch64--glibc--stable-2022.08-1/bin/aarch64-buildroot-linux-gnu-\n\n# Paths for kernel source and build output\nexport NVIDIA_SRC=$TOP_DIR/kernel_sources/Linux_for_Tegra/source\nexport TEGRA_KERNEL_OUT=$NVIDIA_SRC/out/nvidia-linux-header\nexport KERNEL_HEADERS=$NVIDIA_SRC/kernel/kernel-jammy-src\n\n# Path where kernel modules will be installed\nexport INSTALL_MOD_PATH=$LDK_ROOTFS_DIR\n</code></pre>"},{"location":"guides/jetpack/#12-download-required-files","title":"1.2 Download Required Files","text":"<p>Now, let's create the directories and download all the necessary packages from NVIDIA. Bash</p> <pre><code># Create the main directories\nmkdir -p $TOP_DIR/tool_chain\nmkdir -p $TOP_DIR/kernel_sources\n\n\n# Links for JetPack 6.0 (L4T r36.3.0)\nexport TOOL_CHAIN_LINK=\"https://developer.nvidia.com/downloads/embedded/l4t/r36_release_v3.0/toolchain/aarch64--glibc--stable-2022.08-1.tar.bz2\"\nexport L4T_DRIVER_PKG_LINK=\"https://developer.nvidia.com/downloads/embedded/l4t/r36_release_v3.0/release/jetson_linux_r36.3.0_aarch64.tbz2\"\nexport L4T_ROOTFS_LINK=\"https://developer.nvidia.com/downloads/embedded/l4t/r36_release_v3.0/release/tegra_linux_sample-root-filesystem_r36.3.0_aarch64.tbz2\"\nexport DRIVER_PKG_SOURCE_LINK=\"https://developer.nvidia.com/downloads/embedded/l4t/r36_release_v3.0/sources/public_sources.tbz2\"\n</code></pre> <pre><code># Download everything using curl\necho \"Downloading required packages...\"\ncurl -L -o $TOP_DIR/tool_chain/glibc.tar.bz2 $TOOL_CHAIN_LINK\ncurl -L -o $TOP_DIR/Jetson_Linux.tbz2 $L4T_DRIVER_PKG_LINK\ncurl -L -o $TOP_DIR/Sample_rootfs.tbz2 $L4T_ROOTFS_LINK\ncurl -L -o $TOP_DIR/kernel_sources/public_sources.tbz2 $DRIVER_PKG_SOURCE_LINK\necho \"Downloads complete!\"\n</code></pre> <p>e-con Systems Driver Package</p> <p>This guide assumes you have already obtained the camera driver package (e.g., e-CAM200_CUOAGX_JETSON_AGX_ORIN_L4T36.3.0_15-May-2024_R02_RC1.tar.gz) from e-con Systems. You must download it and place it in the $TOP_DIR directory.</p> <p>Example:</p> <pre><code>mv ~/Downloads/JP6.0_L4T36.3.0/e-CAM200_CUOAGX_JETSON_AGX_ORIN_L4T36.3.0_15-May-2024_R02_RC1.tar.gz $TOP_DIR\n</code></pre>"},{"location":"guides/jetpack/#13-extract-files","title":"1.3 Extract Files","text":"<p>Unpack all the downloaded archives.</p> <pre><code># Unzip the e-con Systems driver package\n# Note: The filename might differ. Adjust if necessary.\ncd $TOP_DIR\ntar -xaf e- e-CAM200_CUOAGX_JETSON_AGX_ORIN_L4T36.3.0_15-May-2024_R02_RC1.tar.gz\n\n# Extract the toolchain\ncd $TOP_DIR/tool_chain\ntar -xf glibc.tar.bz2\n\n# Extract Jetson Linux Driver Package and RootFS\ncd $TOP_DIR\ntar xf Jetson_Linux.tbz2\nsudo tar xpf Sample_rootfs.tbz2 -C $LDK_ROOTFS_DIR\n\n# Extract public kernel sources\ncd $TOP_DIR/kernel_sources\ntar xf public_sources.tbz2\n\ncd $NVIDIA_SRC\ntar xf kernel_src.tbz2\ntar xf kernel_oot_modules_src.tbz2\ntar xf nvidia_kernel_display_driver_source.tbz2\n</code></pre>"},{"location":"guides/jetpack/#14-install-required-dependencies","title":"1.4 Install Required Dependencies","text":"<pre><code>sudo apt-get update &amp;&amp; \\\nsudo apt-get install qemu-user-static build-essential \\\nbc lbzip2 flex openssl libssl-dev\n</code></pre>"},{"location":"guides/jetpack/#15-prepare-package-ready-for-flashing","title":"1.5 Prepare Package Ready for Flashing","text":"<pre><code>cd $L4T_DIR\nsudo ./tools/l4t_flash_prerequisites.sh\nsudo ./apply_binaries.sh\n</code></pre>"},{"location":"guides/jetpack/#2-patching-the-kernel","title":"2. Patching the Kernel \ud83e\ude79","text":"<p>With the sources extracted, we apply several patches from the e-con Systems package. This process involves modifying the driver module, the device tree, and NVIDIA's out-of-tree components.</p> <pre><code># Navigate to the main source directory for all patching operations\ncd $NVIDIA_SRC\n</code></pre> <ol> <li> <p>Apply Module Patch: This patch sets up the camera sensor driver to be built out-of-tree.</p> <pre><code>cd $NVIDIA_SRC\nmkdir -p sensor_driver\npatch -d sensor_driver -p1 -i $RELEASE_PACK_DIR/Kernel/e-CAM200_CUOAGX_JETSON_ORIN_L4T36.3.0_module.patch\n</code></pre> </li> <li> <p>Apply Device Tree Patch: Next, we modify and apply the patch for the Device Tree Blob (DTB).</p> <pre><code>sed -i 's/vcc-supply/\\/\\/vcc-supply/g' $RELEASE_PACK_DIR/Kernel/e-CAM200_CUOAGX_JETSON_ORIN_L4T36.3.0_dtb.patch\n\npatch -p1 -i $RELEASE_PACK_DIR/Kernel/e-CAM200_CUOAGX_JETSON_ORIN_L4T36.3.0_dtb.patch\n</code></pre> </li> <li> <p>Apply NVIDIA OOT Patches: Finally, we patch NVIDIA's out-of-tree (OOT) source files. These <code>sed</code> commands are necessary workarounds to ensure the patch applies cleanly.</p> <pre><code>sed '/@@ -2224,6 +2227,26 @@ static long tegra_channel_default_ioctl/,/tegra_channel_close/d' \\\n$RELEASE_PACK_DIR/Kernel/e-CAM200_CUOAGX_JETSON_ORIN_L4T36.3.0_oot.patch &gt; tempfile &amp;&amp; \\\nmv $RELEASE_PACK_DIR/Kernel/e-CAM200_CUOAGX_JETSON_ORIN_L4T36.3.0_oot.patch $RELEASE_PACK_DIR/Kernel/e-CAM200_CUOAGX_JETSON_ORIN_L4T36.3.0_oot.patch.backup &amp;&amp; \\\nmv tempfile $RELEASE_PACK_DIR/Kernel/e-CAM200_CUOAGX_JETSON_ORIN_L4T36.3.0_oot.patch\n\npatch -p1 -i $RELEASE_PACK_DIR/Kernel/e-CAM200_CUOAGX_JETSON_ORIN_L4T36.3.0_oot.patch\n</code></pre> <p>Potential Makefile Patch Failure</p> <p>If the last patch command fails with an error like <code>1 out of 3 hunks FAILED -- saving rejects to file Makefile</code>, you must manually add the following text to the end of the Makefile located in <code>$NVIDIA_SRC</code>.</p> <pre><code>sensor-driver:\n    @if [ ! -d \u201c$(MAKEFILE_DIR)/sensor_driver/$(SENSOR_DRIVER)\u201d ]; then \\\n        echo \u201cDirectory sensor_driver is not found, exiting..\u201d; \\\n        false; \\\n    fi\n\n    @echo \u201c================================================================================\u201d\n    @echo \u201cmake $(MAKECMDGOALS) \u2013 sensor driver \u2026\u201d\n    @echo \u201c================================================================================\u201d\n\n    @if [ -d \u201c$(MAKEFILE_DIR)/sensor_driver/PWM_MCU\u201d ] &amp;&amp; \\\n    [ -d \u201c$(MAKEFILE_DIR)/sensor_driver/$(SENSOR_DRIVER)\u201d ]; then \\\n        $(MAKE) -j $(NPROC) ARCH=arm64 \\\n            CROSS_COMPILE=$(CROSS_COMPILE) \\\n            -C $(NVIDIA_HEADERS) \\\n            M=$(MAKEFILE_DIR)/sensor_driver/PWM_MCU \\\n            srctree.nvconftest=$(NVIDIA_CONFTEST) \\\n            $(MAKECMDGOALS); \\\n      $(MAKE) -j $(NPROC) ARCH=arm64 \\\n          CROSS_COMPILE=$(CROSS_COMPILE) \\\n          -C $(NVIDIA_HEADERS) \\\n          M=$(MAKEFILE_DIR)/sensor_driver/$(SENSOR_DRIVER) \\\n          KBUILD_EXTRA_SYMBOLS=$(MAKEFILE_DIR)/sensor_driver/PWM_MCU/Module.symvers \\\n          srctree.nvconftest=$(NVIDIA_CONFTEST) \\\n          $(MAKECMDGOALS); \\\n    else \\\n        $(MAKE) -j $(NPROC) ARCH=arm64 \\\n            CROSS_COMPILE=$(CROSS_COMPILE) \\\n            -C $(NVIDIA_HEADERS) \\\n            M=$(MAKEFILE_DIR)/sensor_driver/$(SENSOR_DRIVER) \\\n            srctree.nvconftest=$(NVIDIA_CONFTEST) \\\n            $(MAKECMDGOALS); \\\n    fi\n</code></pre> </li> <li> <p>Proper support for camera streaming</p> <pre><code>sed '238,238d' $NVIDIA_SRC/nvidia-oot/drivers/media/platform/tegra/camera/vi/vi5_fops.c &gt; tempfile &amp;&amp; mv tempfile $NVIDIA_SRC/nvidia-oot/drivers/media/platform/tegra/camera/vi/vi5_fops.c\nsed '228i#endif' $NVIDIA_SRC/nvidia-oot/drivers/media/platform/tegra/camera/vi/vi5_fops.c &gt; tempfile &amp;&amp; mv tempfile $NVIDIA_SRC/nvidia-oot/drivers/media/platform/tegra/camera/vi/vi5_fops.c\n</code></pre> </li> </ol>"},{"location":"guides/jetpack/#3-building-kernel","title":"3. Building Kernel \ud83d\udc68\u200d\ud83d\udcbb","text":"<p>Now, we will compile the kernel, modules, and device tree from the main source directory.</p> <pre><code># Ensure you are in the main source directory\ncd $NVIDIA_SRC\n\nmake -C kernel\nmake modules\nsudo -E make modules_install\nmake dtbs\n</code></pre>"},{"location":"guides/jetpack/#4-flash-the-jetson","title":"4. Flash the Jetson \u26a1","text":"<p>The final step is to put the Jetson in recovery mode and flash the entire system.</p>"},{"location":"guides/networking/","title":"Networking","text":"<p>Proper network configuration is essential for reliable and high-throughput communication, especially when working with data-intensive sensors like LiDAR and multiple cameras. To address this, we follow the network setup recommended by the Autoware Foundation, which is tailored for ROS 2 systems with demanding real-time communication needs.</p> <p>We use CycloneDDS as our ROS 2 DDS middleware implementation, with additional system-level tuning to handle the high bandwidth requirements of our sensors.</p>"},{"location":"guides/networking/#autoware-based-configuration","title":"Autoware-Based Configuration","text":"<p>Info</p> <p>The steps below are based on the Autoware documentation. For the original guide, refer to the official Autoware DDS settings page.</p>"},{"location":"guides/networking/#install-and-configure-cyclonedds","title":"Install and Configure CycloneDDS","text":"<ol> <li>Install CycloneDDS for ROS 2 (Humble):</li> </ol> <pre><code>sudo apt install ros-humble-rmw-cyclonedds-cpp\n</code></pre> <ol> <li>Create a CycloneDDS configuration file at <code>~/cyclonedds.xml</code> with the following contents:</li> </ol> <pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt;\n&lt;CycloneDDS xmlns=\"https://cdds.io/config\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"https://cdds.io/config https://raw.githubusercontent.com/eclipse-cyclonedds/cyclonedds/master/etc/cyclonedds.xsd\"&gt;\n  &lt;Domain Id=\"any\"&gt;\n    &lt;General&gt;\n      &lt;Interfaces&gt;\n        &lt;NetworkInterface autodetermine=\"false\" name=\"lo\" priority=\"default\" multicast=\"default\" /&gt;\n      &lt;/Interfaces&gt;\n      &lt;AllowMulticast&gt;default&lt;/AllowMulticast&gt;\n      &lt;MaxMessageSize&gt;65500B&lt;/MaxMessageSize&gt;\n    &lt;/General&gt;\n    &lt;Internal&gt;\n      &lt;SocketReceiveBufferSize min=\"10MB\"/&gt;\n      &lt;Watermarks&gt;\n        &lt;WhcHigh&gt;500kB&lt;/WhcHigh&gt;\n      &lt;/Watermarks&gt;\n    &lt;/Internal&gt;\n  &lt;/Domain&gt;\n&lt;/CycloneDDS&gt;\n</code></pre> <ol> <li>Set CycloneDDS as the default RMW implementation:</li> </ol> <pre><code>echo \"export RMW_IMPLEMENTATION=rmw_cyclonedds_cpp\" &gt;&gt; ~/.bashrc\necho \"export CYCLONEDDS_URI=file:///home/$(whoami)/cyclonedds.xml\" &gt;&gt; ~/.bashrc\n</code></pre>"},{"location":"guides/networking/#system-wide-network-configuration","title":"System-Wide Network Configuration","text":"<p>To support high-throughput data transmission, the following system settings must be adjusted. These changes increase buffer sizes and reduce packet fragmentation delays.</p> <p>Temporary (session-based) configuration:</p> <pre><code># Increase the maximum receive buffer size for network packets\nsudo sysctl -w net.core.rmem_max=2147483647  # 2 GiB, default is 208 KiB\n\n# IP fragmentation settings\nsudo sysctl -w net.ipv4.ipfrag_time=3        # in seconds, default is 30 s\nsudo sysctl -w net.ipv4.ipfrag_high_thresh=134217728  # 128 MiB, default is 256 KiB\n</code></pre> <p>Permanent configuration:</p> <p>Create or edit <code>/etc/sysctl.d/10-cyclone-max.conf</code> with the following:</p> <pre><code># Increase the maximum receive buffer size for network packets\nnet.core.rmem_max=2147483647  # 2 GiB, default is 208 KiB\n\n# IP fragmentation settings\nnet.ipv4.ipfrag_time=3  # in seconds, default is 30 s\nnet.ipv4.ipfrag_high_thresh=134217728  # 128 MiB, default is 256 KiB\n</code></pre> <p>Validate the settings:</p> <pre><code>sysctl net.core.rmem_max net.ipv4.ipfrag_time net.ipv4.ipfrag_high_thresh\n\n# Output\n# net.core.rmem_max = 2147483647\n# net.ipv4.ipfrag_time = 3\n# net.ipv4.ipfrag_high_thresh = 134217728\n</code></pre>"},{"location":"guides/networking/#enable-multicast-on-loopback","title":"Enable Multicast on Loopback","text":"<p>Temporary (until reboot):</p> <pre><code>sudo ip link set lo multicast on\n</code></pre> <p>Permanent via systemd:</p> <ol> <li>Create a systemd unit file at <code>/etc/systemd/system/multicast-lo.service</code>:</li> </ol> <pre><code>[Unit]\nDescription=Enable Multicast on Loopback\n\n[Service]\nType=oneshot\nExecStart=/usr/sbin/ip link set lo multicast on\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <ol> <li>Apply and enable the service:</li> </ol> <pre><code>sudo systemctl daemon-reload\nsudo systemctl enable multicast-lo.service\nsudo systemctl start multicast-lo.service\n</code></pre>"},{"location":"guides/networking/#setup-ouster-needed-for-ptp-support","title":"Setup Ouster (Needed for PTP Support)","text":"<pre><code>sudo apt install dnsmask\n</code></pre> <p>edit file on <code>/etc/dnsmask.d/ouster.conf</code> and add the following</p> <pre><code>port=0\ninterface=eth0\nbind-interfaces\ndhcp-range=192.168.100.10,192.168.100.100,12h\nlog-dhcp\n</code></pre> <p>Configure the link <code>eth0</code> ipv4 to have manual, ip <code>192.168.100.1</code> and 24 as the netmask, or <code>255.255.255.0</code></p> <pre><code>sudo systemctl restart dnsmasq\n</code></pre> <p>connect the ouster, and use <code>arp -a</code> to find ouster IP. The usual os-id.local should also work.</p>"},{"location":"guides/realsense/","title":"Realsense","text":"<p>git clone git@github.com:jetsonhacks/jetson-orin-librealsense.git git clone git@github.com:jetsonhacks/jetson-orin-kernel-builder.git</p> <p>go to jetson-orin-librealsense ./patch-for-realsense.sh</p> <p>go to jetson-oring-kernel-builder ./scripts/get_kernel_sources.sh ./scripts/edit_config_gui.sh For this one go to edit-&gt;find. Search for HID_SENSOR_HUB set value to M by clicking. Search for ACCEL_3D and set value to M. Same for GYRO_3D. </p> <p>go to file -&gt; save close</p> <p>./scripts/make_kernel_modules.sh</p> <p>When finished, type n for declining all modules from being updated</p> <p>sudo cp /usr/src/kernel/kernel-jammy-src/drivers/media/usb/uvc/uvcvideo.ko /lib/modules/5.15.136-tegra/kernel/drivers/media/usb/uvc/uvcvideo.ko sudo cp /usr/src/kernel/kernel-jammy-src/drivers/iio/accel/hid-sensor-accel-3d.ko /lib/modules/5.15.136-tegra/kernel/drivers/iio/accel/hid-sensor-accel-3d.ko sudo cp /usr/src/kernel/kernel-jammy-src/drivers/iio/common/hid-sensors/hid-sensor-iio-common.ko /lib/modules/5.15.136-tegra/kernel/drivers/iio/common/hid-sensors/hid-sensor-iio-common.ko sudo cp /usr/src/kernel/kernel-jammy-src/drivers/hid/hid-sensor-hub.ko /lib/modules/5.15.136-tegra/kernel/drivers/hid/hid-sensor-hub.ko  sudo cp /usr/src/kernel/kernel-jammy-src/drivers/iio/common/hid-sensors/hid-sensor-trigger.ko /lib/modules/5.15.136-tegra/kernel/drivers/iio/common/hid-sensors/hid-sensor-trigger.ko sudo cp /usr/src/kernel/kernel-jammy-src/drivers/iio/gyro/hid-sensor-gyro-3d.ko /lib/modules/5.15.136-tegra/kernel/drivers/iio/gyro/hid-sensor-gyro-3d.ko</p>"},{"location":"guides/setup/","title":"Initial Device Setup","text":"<p>This guide outlines the initial setup process for a new SMapper device. A bootstrap script is provided to automate most of the software installation.</p> <p>It is assumed that the Jetson Orin AGX has already been flashed with Jetpack 6.0.</p> <p>Warning</p> <p>The bootstrap script has never been executed on the real platform. Therefore, the script is higly experimental and could potentially not work first try.</p>"},{"location":"guides/setup/#step-1-bootstrap-the-system","title":"Step 1: Bootstrap the System","text":"<p>The bootstrap script will clone the <code>smapper</code> repository and set up the core software environment. You can run this using one of the two methods below. The automatic method is recommended.</p> <ul> <li> <p>A) Automatic Method (Recommended)</p> <p>Run the following command in your terminal. It will download and execute the bootstrap script directly.</p> <pre><code>curl -fsSL https://raw.githubusercontent.com/snt-arg/smapper/main/bootstrap.sh | sh\n</code></pre> </li> <li> <p>B) Manual Method</p> <p>Alternatively, you can clone the repository manually. It is crucial to use the <code>--recursive</code> flag to ensure all submodules are downloaded.</p> <pre><code>git clone --recursive https://github.com/snt-arg/smapper.git ~/smapper\ncd ~/smapper\n./bootstrap.sh\n</code></pre> </li> </ul>"},{"location":"guides/setup/#step-2-install-e-con-systems-camera-drivers","title":"Step 2: Install e-con Systems Camera Drivers","text":"<p>After the bootstrap is complete, you must patch the kernel for the e-con Systems Argus cameras to work.</p> <p>Note</p> <p>For members of the ARG team, the required driver file (<code>e-CAM200_CUOAGX.zip</code>) is available on the SMapper external disk. Otherwise, the file must be acquired directly from the e-con Systems support portal.</p> <ol> <li> <p>Place the Driver File</p> <p>Copy the <code>e-CAM200_CUOAGX.zip</code> file to the <code>~/tools</code> directory on the device.</p> </li> <li> <p>Install the Binaries</p> <p>Run the following commands to unzip the archive and execute the installation script.</p> <pre><code>cd ~/tools\nunzip e-CAM200_CUOAGX.zip\nsudo ./install_binaries.sh\n</code></pre> </li> </ol> <p>Important</p> <p>The installation script will automatically reboot the device upon completion.</p>"},{"location":"guides/setup/#step-3-verify-camera-installation","title":"Step 3: Verify Camera Installation","text":"<p>After the device reboots, you can verify that the cameras are detected correctly. Run the following commands:</p> <ol> <li> <p>Check the system message log for camera detection:</p> <pre><code>sudo dmesg | grep \"detected\"\n</code></pre> </li> <li> <p>Confirm that a video device now exists:     <pre><code>ls /dev/video0\n</code></pre>     If this command succeeds, the driver is working. You can also run the <code>eCAM_argus_camera</code> executable to test the camera stream.</p> </li> </ol>"},{"location":"guides/setup/#step-4-configure-network-hotspot","title":"Step 4: Configure Network Hotspot","text":"<p>The final step is to create a Wi-Fi hotspot so you can connect to the SMapper Web App from another computer or mobile device.</p> <ol> <li> <p>Create the Hotspot</p> <p>The simplest method is to use the desktop GUI. Navigate to your system's Wi-Fi Settings and follow the prompts to create and enable a new hotspot.</p> </li> <li> <p>Set Hotspot to Start on Boot (Optional)</p> <p>To ensure the hotspot is always available after a reboot, you can set its connection profile to be the default.</p> <ul> <li>Navigate to <code>/etc/NetworkManager/system-connections/</code>.</li> <li>Edit the file corresponding to your hotspot's name (e.g., <code>Hotspot.nmconnection</code>).</li> <li>Inside the file, under the <code>[connection]</code> section, ensure the <code>autoconnect</code> property is set to <code>true</code>:     <pre><code>[connection]\nid=YourHotspotName\n...\nautoconnect=true\n</code></pre></li> </ul> </li> </ol>"},{"location":"guides/setup/#step-5-final-verification","title":"Step 5: Final Verification","text":"<p>Once the hotspot is active, connect to its Wi-Fi network from your laptop or phone. Open a web browser and navigate to https://10.42.0.1</p> <p>If the setup was successful, you should see the SMapper web application interface.</p>"},{"location":"software_stack/","title":"\ud83e\udde0 Software Stack Overview","text":"<p>The brains of SMapper</p> <p>SMapper runs a robust and modular software stack designed to handle the full pipeline of data acquisition, real-time processing, visualization, and storage\u2014all on-device, thanks to the onboard Jetson AGX Orin Developer Kit (64GB).</p> <p>The stack brings together ROS 2 nodes, custom software packages, and web-based interfaces to provide a streamlined experience for both field operations and remote monitoring.</p>"},{"location":"software_stack/#system-runtime","title":"\u2699\ufe0f System Runtime","text":"<ul> <li> <p>Base OS &amp; Platform   The system runs JetPack 6.0, chosen to maintain compatibility with the e-Con Systems Argus cameras, which require a custom kernel only supported by this version.</p> </li> <li> <p>ROS 2 Distribution   All robot-centric components operate on ROS 2 Humble, ensuring long-term support and compatibility across the ecosystem.</p> </li> <li> <p>Sensor Data Flow</p> <ul> <li>Sensor drivers (LiDAR, cameras, IMUs) publish to ROS topics.</li> <li>A LiDAR odometry node estimates pose in real-time.</li> <li>Data is recorded into ROS 2 bag files for offline processing.</li> </ul> </li> </ul>"},{"location":"software_stack/#api-web-interface","title":"\ud83c\udf10 API &amp; Web Interface","text":"<p>Alongside the ROS stack, SMapper hosts a local hotspot that runs:</p> <ul> <li>A REST API to control data capture, monitor system status, and trigger recordings.</li> <li>A web application for real-time visualization, interaction, and metadata management.</li> <li>This very documentation site, served locally for offline access in the field.</li> </ul> <p>These services communicate with ROS through a ROS bridge, allowing real-time updates (e.g., live point clouds or pose trajectories) to appear directly in the browser.</p>"},{"location":"software_stack/#key-components","title":"\ud83e\udde9 Key Components","text":"Component Description <code>ouster_ros</code> Official Ouster ROS 2 driver used to interface with the OS0-64 LiDAR. <code>argus_ros</code> (custom) Custom-built ROS 2 package for interfacing with 4x Argus 20MP cameras. <code>mola_lidar_odometry</code> LiDAR odometry package used for pose estimation. <code>rosbridge_server</code> WebSocket and JSON-based bridge for connecting the web app to ROS. <code>smapper_api</code> RESTful API that exposes control and status endpoints. <code>smapper_app</code> Web dashboard and visualization layer built for touch-friendly interaction."},{"location":"software_stack/#learn-more","title":"\ud83d\udd0d Learn More","text":"<p>You can explore each software module in more detail:</p> <ul> <li>SMapper API</li> <li>SMapper Web App</li> <li>Argus Cameras ROS Package</li> </ul>"},{"location":"software_stack/argus_cameras_ros/","title":"Argus Cameras ROS Package","text":"<p>The <code>argus_camera_ros</code> package provides a custom C++ ROS driver for NVIDIA Jetson platforms, specifically designed for high-performance applications where precise timestamping and low latency are critical. This driver was developed as an alternative to existing solutions that couldn't meet the demanding requirements for real-time robotic systems.</p>"},{"location":"software_stack/argus_cameras_ros/#features","title":"Features","text":"<ul> <li>Event-Driven Architecture: Each camera is treated as an independent object, minimizing latency and maximizing performance.</li> <li>Precise Timestamping: The driver captures hardware timestamps for each frame, ensuring accuracy.</li> <li>Time Synchronization: The driver supports two primary time synchronization modes:<ul> <li><code>TIME_FROM_ROS</code> (Default): The system uses the ROS time domain. This is the default operational mode and provides reasonable accuracy due to the low-latency driver design.</li> <li><code>TIME_FROM_PTP</code> (Precision Time Protocol): This mode allows the driver to convert hardware timestamps into the PTP time domain. It is designed to work with a setup where the Jetson is the PTP master.</li> </ul> </li> </ul>"},{"location":"software_stack/argus_cameras_ros/#system-requirements","title":"System Requirements","text":"<ul> <li>ROS: Humble</li> <li>OS: Jetpack 6.0 (or newer, but has not been tested on versions beyond Jetpack 6.0)</li> <li>Hardware: NVIDIA Jetson platform with a compatible camera.</li> </ul>"},{"location":"software_stack/argus_cameras_ros/#getting-started","title":"Getting Started","text":""},{"location":"software_stack/argus_cameras_ros/#building-and-installation","title":"Building and Installation","text":"<p>Note: This package is designed to be built and run exclusively on NVIDIA Jetson platforms with the specified Jetpack versions.</p> <ol> <li> <p>Clone the repository into your ROS workspace.</p> <pre><code>cd &lt;your_ros_workspace&gt;/src\ngit clone [https://github.com/snt-arg/argus_camera_ros.git](https://github.com/snt-arg/argus_camera_ros.git)\n</code></pre> </li> <li> <p>Build the package using <code>colcon</code>.</p> <pre><code>cd &lt;your_ros_workspace&gt;\ncolcon build --packages-select argus_camera_ros\n</code></pre> </li> </ol>"},{"location":"software_stack/argus_cameras_ros/#using-ptp-timestamping","title":"Using PTP Timestamping","text":"<p>To use the <code>TIME_FROM_PTP</code> timestamping mode, you must first ensure that the system's PTP configuration is correct and then enable read/write access to the PTP hardware clock.</p> <ol> <li> <p>Edit the udev rules to grant permissions to the PTP device.</p> <pre><code>sudo vim /etc/udev/rules.d/99-nvpps.rules\n</code></pre> </li> <li> <p>Add the following line to the file:</p> <pre><code>KERNEL==\"nvpps0\", MODE=\"0666\"\n</code></pre> </li> <li> <p>Apply the new udev rules by running the following commands or by rebooting your system.</p> <pre><code>sudo udevadm control --reload\nsudo udevadm trigger\n</code></pre> </li> </ol> <p>With these steps complete, you can now configure the <code>argus_camera_ros</code> package to use the <code>TIME_FROM_PTP</code> timestamping mode.</p>"},{"location":"software_stack/smapper_api/","title":"SMapper API","text":"<p>The SMapper API is a FastAPI-based application that offers a RESTful interface for controlling the SMapper device or other similar systems. The primary goal of this API is to provide a straightforward, web-based interface to manage services, such as ROS2 nodes or simple shell commands.</p>"},{"location":"software_stack/smapper_api/#key-features","title":"Key Features","text":"<ul> <li>Service Management: Easily start and stop background services (e.g., ROS2 nodes) through simple API endpoints.</li> <li>ROS Bag Recording: Manage ROS bag recordings and automatically track metadata (like topics, duration, and file size) in a local SQLite database.</li> <li>ROS Topic Monitoring: Monitor available ROS topics and retrieve metadata such as their status, message type, and publishing frequency (Hz).</li> <li>Extensible Configuration: Define custom services, commands, and topic presets through simple YAML configuration files.</li> </ul>"},{"location":"software_stack/smapper_api/#prerequisites-installation","title":"Prerequisites &amp; Installation","text":"<p>Before you begin, ensure your system meets the following requirements.</p>"},{"location":"software_stack/smapper_api/#1-ros2-humble","title":"1. ROS2 Humble","text":"<p>The API is built to interface directly with ROS2 services. You must have ROS2 Humble Hawksbill installed on your system.</p> <ul> <li>You can find the official installation instructions at docs.ros.org/en/humble/Installation.</li> </ul>"},{"location":"software_stack/smapper_api/#2-uv-package-manager","title":"2. UV Package Manager","text":"<p>This project uses UV to manage its Python dependencies. UV is a fast package manager from Astral.</p> <ol> <li>Install UV: If you don't have it, you can install UV by following the instructions here.</li> <li>Install Dependencies: Navigate to the project's root directory and run any <code>uv sync</code> command. UV will automatically create a virtual environment and install all required packages listed in <code>pyproject.toml</code>.</li> </ol>"},{"location":"software_stack/smapper_api/#configuration","title":"Configuration","text":"<p>The API's behavior is controlled by YAML configuration files located in the <code>config/</code> directory.</p> <ul> <li> <p>Default Configuration: By default, the API loads its settings from <code>config/settings.yaml</code>. This file is where you can define available services, the commands they execute, and presets for ROS bag recordings.</p> </li> <li> <p>Custom Configurations: You can create multiple configuration files (e.g., <code>config/another_settings.yaml</code>). To instruct the API to use a different file, set the <code>API_SETTINGS_NAME</code> environment variable to the name of your file (without the <code>.yaml</code> extension).</p> <p>For example:</p> <pre><code>export API_SETTINGS_NAME=another_settingsl.yaml\n# Now when the API starts, it will load config/another_settings.yaml\n</code></pre> </li> </ul>"},{"location":"software_stack/smapper_api/#usage","title":"Usage","text":"<p>Note</p> <p>All commands should be executed from the root directory of the project.</p> <p>The API can be run in two modes: development and production.</p>"},{"location":"software_stack/smapper_api/#development-mode","title":"Development Mode","text":"<p>For development, use the <code>dev</code> command. This will start the server with hot-reloading enabled, which automatically restarts the server whenever you make changes to the code.</p> <pre><code>uv run fastapi dev\n</code></pre>"},{"location":"software_stack/smapper_api/#production-mode","title":"Production Mode","text":"<p>For production environments, use the run command. This starts the API using the unicorn server, which is optimized for stability and performance.</p> <pre><code>uv run fastapi run\n</code></pre>"},{"location":"software_stack/smapper_api/#deployment","title":"Deployment","text":"<p>For deploying the API as a service that starts automatically on system boot, a helper script is provided.</p> <p>The <code>deploy.sh</code> script, located in the project's root directory, is designed to be used by a systemd service or a similar init system. It sets up the necessary environment and launches the API in production mode, ensuring it is running after the device boots up.</p>"},{"location":"software_stack/smapper_app/","title":"SMapper Web App","text":"<p>The SMapper Web App is a modern, responsive user interface designed to interact with the SMapper API. It provides a user-friendly way to monitor, control, and manage the SMapper device's services and data recordings.</p> <p>The application is built with React on top of the Vite tooling ecosystem and uses the Bun runtime for exceptional performance. Component design and layout are handled by the Chakra UI library, which enables rapid and consistent development.</p>"},{"location":"software_stack/smapper_app/#key-features","title":"Key Features","text":"<p>The application is organized into four main pages:</p> <ul> <li>Dashboard: Provides a central overview of the system's status.</li> <li>Visualizer: Embeds a <code>rosboard</code> instance, allowing for real-time visualization of ROS topics such as camera image streams and 3D point clouds.</li> <li>Services: Allows users to start and stop the various ROS2 nodes and other background services defined in the SMapper API's configuration.</li> <li>Recordings: Provides an interface to start, stop, and manage ROS bag recordings, listing previous recordings and their metadata.</li> </ul>"},{"location":"software_stack/smapper_app/#technology-stack","title":"Technology Stack","text":"<ul> <li>Runtime: Bun</li> <li>Bundler: Vite</li> <li>Framework: React</li> <li>UI Library: Chakra UI</li> <li>Web Server (in Docker): Caddy</li> </ul>"},{"location":"software_stack/smapper_app/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have Bun installed on your system.</p> <ul> <li>You can find the official installation instructions at bun.sh/docs/installation.</li> </ul>"},{"location":"software_stack/smapper_app/#configuration","title":"Configuration","text":"<p>The application is configured using environment variables. Create a <code>.env</code> file in the root of the project and populate it with the following variables as needed:</p> Variable Description Required Default <code>VITE_API_BASE_URL</code> The base URL for the SMapper API endpoint. Yes - <code>VITE_ROSBOARD_URL</code> The URL for the <code>rosboard</code> instance that will be embedded in the Visualizer page. Yes - <code>VITE_SERVICES_POLLING_INTERVAL</code> The interval (in ms) at which the app polls the API for the status of services. No <code>1000</code> <code>VITE_TOPICS_POLLING_INTERVAL</code> The interval (in ms) at which the app polls the API for the list of available ROS topics. No <code>3000</code> <code>VITE_RECORDING_STATE_POLLING_INTERVAL</code> The interval (in ms) at which the app polls the API for the current recording state. No <code>1000</code> <code>VITE_MOCK_API</code> Set to <code>true</code> to use mocked data instead of calling the real API. Used for UI development and display. No -"},{"location":"software_stack/smapper_app/#local-development-usage","title":"Local Development &amp; Usage","text":"<p>Note</p> <p>All commands should be executed from the root directory of the project.</p>"},{"location":"software_stack/smapper_app/#run-in-development-mode","title":"Run in Development Mode","text":"<p>Starts the development server with hot-reloading.</p> <pre><code>bun run dev\n</code></pre>"},{"location":"software_stack/smapper_app/#build-for-production","title":"Build for Production","text":"<p>Bundles and optimizes the application for production. The output is placed in the dist/ directory.</p> <pre><code>bun run build\n</code></pre>"},{"location":"software_stack/smapper_app/#preview-production-build","title":"Preview Production Build","text":"<p>Starts a local server to preview the production build from the dist/ directory.</p> <pre><code>bun run preview\n</code></pre>"},{"location":"software_stack/smapper_app/#deployment","title":"Deployment","text":"<p>Deployment is handled using Docker and Docker Compose. The setup uses a multi-stage <code>Dockerfile</code> to create a lean production image served by the high-performance Caddy web server.</p> <p>The <code>docker-compose.yml</code> file defines two services for building the web app:</p> <ul> <li> <p><code>app_dev</code>: Builds the app for use within a <code>localhost</code> environment.</p> </li> <li> <p><code>app_prod</code>: Builds the app to be exposed on the local network (e.g., when the device is running a Wi-Fi hotspot).</p> </li> </ul>"},{"location":"software_stack/smapper_app/#build-process","title":"Build Process","text":"<p>To build the Docker image, you must pass the <code>API_URL</code> and <code>ROSBOARD_URL</code> as build arguments. Bash</p> <pre><code># Example for building the production service\nAPI_URL=\"http://10.0.0.1:8000/api/v1\" \\\nROSBOARD_URL=\"http://10.0.0.1:8888\" \\\ndocker-compose build app_prod\n</code></pre> <p>After the build is complete, you can start the service using <code>docker-compose up app_prod</code>.</p>"},{"location":"software_stack/smapper_toolbox/","title":"SMapper Toolbox","text":"<p>The SMapper Toolbox is a command-line interface (CLI) built with Python to simplify the calibration process for the SMapper device. It automates complex procedures, reducing them to a handful of simple commands.</p>"},{"location":"software_stack/smapper_toolbox/#features","title":"Features","text":"<ul> <li>Automated Calibration: Executes the Kalibr toolbox within Docker containers to handle camera intrinsics, IMU noise models, and camera-IMU extrinsics.</li> <li>ROS Bag Conversion: Automatically converts ROS2 bags to the ROS1 format required by Kalibr.</li> <li>TF Tree Generation: Creates a complete ROS transformation (TF) tree from the calibration results.</li> <li>Configuration Conversion: Transforms Kalibr's output into standard ROS <code>camera_info</code> configuration files.</li> </ul>"},{"location":"software_stack/smapper_toolbox/#how-it-works","title":"How It Works","text":"<p>The toolbox streamlines calibration by spawning Docker containers with the Kalibr environment. Since Kalibr requires ROS1, the toolbox first initiates a conversion process to transform your ROS2 rosbags into the compatible ROS1 format before running the calibration tasks.</p>"},{"location":"software_stack/smapper_toolbox/#prerequisites","title":"Prerequisites","text":"<p>Before using the toolbox, ensure you have met the following requirements.</p>"},{"location":"software_stack/smapper_toolbox/#1-uv-package-manager","title":"1. UV Package Manager","text":"<p>The <code>smapper_toolbox</code> uses UV, an extremely fast Python package manager from Astral.</p> <ul> <li>Installation (Linux/macOS): <pre><code>curl -LsSf [https://astral.sh/uv/install.sh](https://astral.sh/uv/install.sh) | sh\n</code></pre></li> <li>You can find more information about UV on its official website and GitHub repository.</li> </ul>"},{"location":"software_stack/smapper_toolbox/#2-rosbag-folder-structure","title":"2. Rosbag Folder Structure","text":"<p>For the calibration commands, your rosbags must be organized in the following directory structure. The toolbox looks for a root folder containing a <code>ros2</code> subdirectory where the bags are located.</p> <pre><code>.\n\u2514\u2500\u2500 ros2\n    \u251c\u2500\u2500 bag1\n    \u251c\u2500\u2500 bag2\n</code></pre>"},{"location":"software_stack/smapper_toolbox/#getting-started","title":"Getting Started","text":""},{"location":"software_stack/smapper_toolbox/#configuration","title":"Configuration","text":"<p>The primary configuration file is located at <code>config/config.yaml</code>. Here, you can define settings for the calibration pipeline, such as workspace parameters.</p> <p>Alternatively, many settings can be overridden directly with command-line options. For example: <code>toolbox kalibr [subcommand] --option-name value</code></p> <p>Some commands, like <code>cam_info</code>, require input and output files to be specified via options: <code>toolbox cam_info generate --input &lt;input_file&gt; --output &lt;output_file&gt;</code></p>"},{"location":"software_stack/smapper_toolbox/#usage","title":"Usage","text":"<ol> <li>Navigate to the root directory of the <code>smapper_toolbox</code> project.</li> <li> <p>Run the help command. On the first run, UV will automatically download and install all required dependencies into a virtual environment.</p> <pre><code>uv run ./toolbox --help\n</code></pre> </li> <li> <p>This will display the main help menu, showing all available commands and options:</p> <pre><code>\u276f uv run ./toolbox --help\nINFO:     Hello from calib-toolbox!\n\n Usage: toolbox [OPTIONS] COMMAND [ARGS]...\n\n A toolbox to help automate the calibration process using dockerised kalibr.\n\n This tool helps calibrate cameras and IMUs using the Kalibr toolbox in a Docker container. It supports\n camera calibration, IMU calibration, and camera-IMU calibration.\n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --install-completion           Install completion for the current shell.                                \u2502\n\u2502 --show-completion              Show completion for the current shell, to copy it or customize the       \u2502\n\u2502                                installation.                                                            \u2502\n\u2502 --help                         Show this message and exit.                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Commands \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 kalibr                                                                                                  \u2502\n\u2502 transforms                                                                                              \u2502\n\u2502 cam_info                                                                                                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> </li> </ol>"}]}